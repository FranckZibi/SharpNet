{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8202f5",
   "metadata": {},
   "source": [
    "# Common Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d009d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "log_file_name = str( Path(os.path.abspath(''))  /  \"PyTorch.log\" )\n",
    "print(f'log_file_name: {log_file_name}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "\n",
    "\n",
    "#2_1_4_4\n",
    "#x_train = np.array([[[[0.7262433,0.8173254,0.7680227,0.5581612],[0.2060332,0.5588848,0.9060271,0.4421779],[0.9775497,0.2737045,0.2919063,0.4673147],[0.6326591,0.4695119,0.9821513,0.03036699]]],[[[0.8623701,0.9953471,0.6771811,0.3145918],[0.8169079,0.8480518,0.9919022,0.0326252],[0.699942,0.5262842,0.9340187,0.6876203],[0.5468155,0.08110995,0.1871246,0.4533272]]]], float)\n",
    "y_2_3 = np.array([[1,0,0],[0,0,1]], float)\n",
    "#1_1_4_4\n",
    "#x_train = np.array([[[[0.7262433,0.8173254,0.7680227,0.5581612],[0.2060332,0.5588848,0.9060271,0.4421779],[0.9775497,0.2737045,0.2919063,0.4673147],[0.6326591,0.4695119,0.9821513,0.03036699]]]], float)\n",
    "#x_train = np.array([[[[1,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]]], float)\n",
    "#y_train = np.array([[1,0,0]], float)\n",
    "#1_1_2_2\n",
    "#x_train = np.array([[[[0.7262433,0.8173254],[0.2060332,0.5588848]]]], float)\n",
    "#1_2_1_1\n",
    "#x_train = np.array([[[[0.75]],[[0.25]]]], float)\n",
    "#1_3_1_1\n",
    "#x_train = np.array([[[[1.0]],[[0.5]],[[0.0]]]], float)\n",
    "#1_2_1_1\n",
    "#x_train = np.array([[[[1.0]],[[0.5]]]], float)\n",
    "#1_2_2_2\n",
    "#x_train = np.array([[[[0.0,0.1],[0.2,0.3]],[[0.5,0.6],[0.7,1.0]]]], float)\n",
    "#2_3_4_5\n",
    "x_2_3_4_5 = np.array([[[[0.67872983,0.95197606,0.8040681,0.17448357,-0.88190055],[0.17665438,1.2180812,-0.17346638,1.4326493,-0.67888665],[-0.62428117,-0.0980559,0.39797723,-0.09146436,1.4464538],[-1.4088991,1.0871105,1.4860412,0.53154343,-0.55622464]],[[0.9507237,1.0441554,1.4757066,-1.4021244,0.599826],[0.07885243,1.302056,0.56286085,0.1404463,-1.2566701],[-0.9386263,-0.14001845,-0.6084844,1.4656314,0.42809242],[0.7888908,-1.4088172,-0.3569865,-0.47057447,1.3723655]],[[0.0153876385,0.6479175,-1.1431268,-0.67964554,1.2212938],[0.8842968,-0.48851898,-0.12837364,-1.0595249,-0.83605576],[-0.26978016,0.6561805,0.35949084,-0.036095843,-0.9152597],[1.1345744,0.97626954,0.7061925,1.0746577,0.53924704]]],[[[0.3745984,-0.8447797,1.1860874,1.1893194,-1.2402604],[1.0157373,-0.9895715,0.43234587,0.98044324,-0.842437],[1.4999181,-0.05590306,0.45661572,0.65917796,0.92838776],[-1.1971939,0.68728215,0.65535206,-1.1234182,1.2155292]],[[-0.9326286,-0.069570385,0.122385725,-0.52349794,0.51311],[-0.094806775,1.0004907,0.9276114,0.880891,0.79351795],[1.3126212,-0.6150096,0.068355896,0.4901785,-0.5022329],[-0.63983274,0.9618302,-0.8324462,-0.9393852,-1.2944435]],[[0.3738683,0.5791351,0.39118314,1.2829121,-0.83597386],[1.2861229,-1.3004352,1.2003129,0.53551644,-1.2180659],[-1.0527077,-1.1790825,0.32961074,1.3591285,-0.028124375],[-1.0558312,0.53283465,0.20958523,-0.8237906,0.35454643]]]], float)\n",
    "#1_3_2_2\n",
    "#x_train = np.array([[[[0.0,0.1],[0.2,0.3]] , [[0.4,0.5],[0.6,0.7]], [[0.8,0.9],[0.95,1.0]] ]], float)\n",
    "#x_train = np.array([ [1,2] , [3,4]] , float)\n",
    "\n",
    "#x_train = np.array([[[[0.0,0.0],[0.0,0.0]] , [[0.0,0.0],[0,0]], [[0.0,0],[0.0,1]]]], float)\n",
    "#x_train = np.array([[ [[0.0,0],[0.0,1]]]], float)\n",
    "\n",
    "\n",
    "#1_2_2_2\n",
    "#x_train = np.array([ [[[0.0,0.1],[0.2,0.3]],[[0.5,0.6],[0.7,1.0]]]], float)\n",
    "#y_train = np.array([[1,0]], float)\n",
    "#y_train = np.array([[1,0]], float)\n",
    "#2_2\n",
    "y_2_2  = np.array([[1,0],[1,0]], float)\n",
    "#1_1_2_1\n",
    "#x_train = np.array([[[[0.7262433],[0.2060332]]]], float)\n",
    "#y_train = np.array([[1,0]], float)\n",
    "#y_train = np.array([[1,0,0,0,0,0]], float)\n",
    "\n",
    "#1_1_1_1\n",
    "#x_train = np.array([[[[1.0]]]], float)\n",
    "#y_train = np.array([[1,0]], float)\n",
    "\n",
    "def truncate(s, max_size):\n",
    "    if len(s)<=max_size:\n",
    "        return s\n",
    "    return s[:(max_size//2)]+\"...\"+s[-(max_size//2):]\n",
    "\n",
    "def log(tensor, tensor_name = \"\", display_to_screen = True):\n",
    "    with torch.no_grad():\n",
    "        if isinstance(tensor, torch.nn.parameter.Parameter) or isinstance(tensor, torch.Tensor):\n",
    "            return log(tensor.detach().to('cpu').numpy(), tensor_name, display_to_screen)\n",
    "        if isinstance(tensor, list):\n",
    "            for e in tensor:\n",
    "                log(e, tensor_name, display_to_screen)\n",
    "            return\n",
    "        asString = None\n",
    "        if isinstance(tensor,str):\n",
    "            asString = tensor\n",
    "        elif isinstance(tensor,np.ndarray):\n",
    "            asString = str(tensor_name) + \" \" + str(tensor.shape) + \"\\n\" + truncate(str(tensor.tolist()), 10000)\n",
    "        else:\n",
    "            asString = str(tensor)\n",
    "        with open(log_file_name, \"a\") as myfile:\n",
    "            myfile.write(asString + \"\\n\")\n",
    "        if display_to_screen:\n",
    "            print(asString)\n",
    "\n",
    "def print_intermediate_layer_model(layer, layer_name, layer_input: torch.Tensor, display_to_screen) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            if layer_input is None:\n",
    "                return None\n",
    "            if not isinstance(layer, torch.nn.Module):\n",
    "                raise Exception(f'not a layer {type(layer)}  {layer}')            \n",
    "            layer_output = layer.forward(layer_input)\n",
    "            log(layer_output, layer_name, display_to_screen)\n",
    "            return layer_output\n",
    "        except Exception as e:  \n",
    "            #print(e)\n",
    "            return None\n",
    "\n",
    "def print_model(model, inputs, display_weights = True, display_layers_output = False, display_layers_grads = False, display_to_screen = False):\n",
    "    if not isinstance(model, torch.nn.Module):\n",
    "        raise Exception(f'not a model{type(model)}  {model}')            \n",
    "    with torch.no_grad():\n",
    "        layers = list(model.named_modules())[1:]\n",
    "\n",
    "        layer_input = inputs\n",
    "        for layerIndex in range(len(layers)):\n",
    "            if layerIndex>3 and layerIndex <  (len(layers)-3 and False)  :\n",
    "                continue\n",
    "            layer_name = layers[layerIndex][0]\n",
    "            layer = layers[layerIndex][1]\n",
    "            log(\"--------------------------------------------------------------------\\n-\",\"\", display_to_screen)           \n",
    "            log(\"layer#\"+ str(layerIndex)+\": \"+layer_name, \"\", display_to_screen)\n",
    "            #log(str(layer.get_config()), \"get_config:\", False)\n",
    "            if display_weights:\n",
    "                for p_name, p in layer.named_parameters():\n",
    "                    log(p, layer_name+\".\"+p_name, display_to_screen)\n",
    "            if display_layers_grads:\n",
    "                for p_name, p in layer.named_parameters():\n",
    "                    if p.grad is not None:\n",
    "                        log(p.grad, layer_name+\".\"+p_name+\" grad:\", display_to_screen)\n",
    "            log(\"-\", \"\", display_to_screen)\n",
    "            if display_layers_output:\n",
    "                log(\"layer#\"+ str(layerIndex)+\" output:\", \"\", display_to_screen)\n",
    "                layer_input = print_intermediate_layer_model(layer, layer_name, layer_input, display_to_screen)\n",
    "        log(\"--------------------------------------------------------------------\", display_to_screen=display_to_screen)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "def Train(model: torch.nn.Module, \n",
    "          X_numpy: np.array,\n",
    "          Y_numpy: np.array,\n",
    "          lr: float,\n",
    "          device: str,\n",
    "          loss_criterion,\n",
    "          optimizer,\n",
    "          numEpochs: int,\n",
    "          batch_size: int|None = None,\n",
    "          momentum: float = 0.0,\n",
    "          lambdaL2Regularization: float = 0.0,\n",
    "         ) -> (float, float):\n",
    "\n",
    "    # by default, the batch size will proceed all elements at once\n",
    "    \n",
    "    if not batch_size or batch_size <= 0:\n",
    "        batch_size = X_numpy.shape[0]\n",
    "    \n",
    "    if os.path.isfile(log_file_name):\n",
    "        try:\n",
    "            os.remove(log_file_name)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    X = torch.tensor(X_numpy, device=device, dtype=torch.float32)\n",
    "    Y_true = torch.tensor(Y_numpy, device=device, dtype=torch.float32)\n",
    "    dataset = TensorDataset(X, Y_true) \n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    log( \"--------------------------------------------------------------------\\n-\")\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        summary(model, input_size=(X.shape[1:]), device = device)\n",
    "    model_summary = f.getvalue()\n",
    "    log(str(model_summary), '', True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        Y_pred_before = model.forward(X)\n",
    "        train_loss_before = loss_criterion(Y_pred_before, Y_true).to('cpu').item()\n",
    "\n",
    "    log( \"-\\n--------------------------------------------------------------------\\n-\", \"\", False)\n",
    "    log(X,\"X\", False)\n",
    "    log(Y_true,\"Y_true\", False)\n",
    "    log(\"--------------------------------------------------------------------\\nmodel before trainig\",\"\", False)\n",
    "    #log( \"--------------------------------------------------------------------\\nprint_layers(model) before\\n-\")\n",
    "    print_model(model, X, display_weights = True, display_layers_output = True, display_to_screen = False)\n",
    "\n",
    "\n",
    "\n",
    "    print( \"-\")\n",
    "    print( \"--------------------------------------------------------------------\")\n",
    "    print( \"-\")\n",
    "    print( \"training for \",numEpochs,\" epochs...\")\n",
    "\n",
    "    log( \"-\\n--------------------------------------------------------------------\\n-\\nmodel after trainig\")\n",
    "    # log( \"--------------------------------------------------------------------\\nprint_layers(model) after\\n-\")\n",
    "\n",
    "\n",
    "    for epoch in range(0,numEpochs):\n",
    "        for batch_id, (x_batch,y_batch_true) in enumerate(data_loader):\n",
    "            y_batch_pred = model(x_batch)\n",
    "            l = loss_criterion(y_batch_pred, y_batch_true) # compute loss\n",
    "            #with torch.no_grad():\n",
    "            #    log(f'loss for epoch#{epoch} / batch_id#{batch_id} : {l.cpu().item()}', '', True)\n",
    "            l.backward() # compute gradients\n",
    "            optimizer.step() # update weights\n",
    "\n",
    "            if epoch == (numEpochs-1):\n",
    "                with torch.no_grad():\n",
    "                    print_model(model, X, display_weights = False, display_layers_grads = True ,display_layers_output = False, display_to_screen = False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Y_pref_after = model.forward(X)\n",
    "        train_loss_after = loss_criterion(Y_pref_after, Y_true).to('cpu').item()\n",
    "\n",
    "    print_model(model, X, display_weights = True, display_layers_output = True, display_to_screen = False)\n",
    "\n",
    "    log(\"PyTorch numEpochs = \" + str(numEpochs))\n",
    "    # log(\"TF learningRate = \"+str(keras.backend.eval(optim.lr)))\n",
    "    log(\"PyTorch lr = \" + str(lr))\n",
    "    log(\"PyTorch momentum = \" + str(momentum))\n",
    "    log(\"PyTorch batch_size = \" + str(batch_size))\n",
    "    log(\"PyTorch lambdaL2Regularization = \" + str(lambdaL2Regularization))\n",
    "    #log(\"PyTorch use_bias = \" + str(use_bias))\n",
    "    log(Y_pred_before, \"PyTorch Y_pred_before\", True)\n",
    "    log(\"PyTorch loss_before=\"+str(train_loss_before))\n",
    "    log(Y_pref_after, \"PyTorch Y_pref_after\", True)\n",
    "    log(\"PyTorch loss_after=\"+str(train_loss_after))\n",
    "    return train_loss_before, train_loss_after\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a28a32",
   "metadata": {},
   "source": [
    "# Compute Gradient in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "input = torch.tensor( [[0.01995766907930374, 0.05039015784859657, 0.04157894104719162]], requires_grad=True)\n",
    "target = torch.tensor([[1.0, 0, 0]])\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "input.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094e90a",
   "metadata": {},
   "source": [
    "# Compute Gradient in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54221f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Define your predictions and ground truth\n",
    "predictions = tf.constant([[0.01995766907930374, 0.05039015784859657, 0.04157894104719162]])\n",
    "ground_truth = tf.constant([[1.0, 0, 0]])\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) loss\n",
    "mse_loss = tf.reduce_mean(tf.square(predictions - ground_truth))\n",
    "\n",
    "# Use GradientTape to compute the gradient\n",
    "with tf.GradientTape() as tape:\n",
    "    # Watch the variables we want to compute gradients with respect to\n",
    "    tape.watch(predictions)\n",
    "    tape.watch(ground_truth)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = tf.reduce_mean(tf.square(predictions - ground_truth))\n",
    "\n",
    "# Compute the gradient of the loss with respect to the predictions\n",
    "gradients = tape.gradient(loss, predictions)\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68cb1f8",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestParallelRunWithPyTorch_Mse(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.dense = torch.nn.Linear(5, 3)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.flatten_dense_1 = torch.nn.Flatten(1)\n",
    "        self.dense_1 = torch.nn.Linear(3*4*3, 3)\n",
    "        torch.nn.init.zeros_(self.dense_1.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_dense = self.dense(x)\n",
    "        y_relu = self.relu(y_dense)\n",
    "        y_flatten_dense_1 = self.flatten_dense_1(y_relu)\n",
    "        y_dense_1 = self.dense_1(y_flatten_dense_1)\n",
    "        return y_dense_1\n",
    "\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "device = 'cuda'\n",
    "# lambdaL2Regularization = 0.00;\n",
    "model = TestParallelRunWithPyTorch_Mse().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = momentum)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      x_2_3_4_5,\n",
    "      y_2_3, \n",
    "      lr = lr, \n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.MSELoss(), \n",
    "      optimizer = optimizer, \n",
    "      numEpochs = 10, \n",
    "      momentum = momentum\n",
    "      )\n",
    "\n",
    "assert abs(loss_before-0.2612681984901428) < 1e-6\n",
    "assert abs(loss_after-0.006385428365319967) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f65f26",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567cf1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "class Multiply(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,  mainMatrix : torch.Tensor, diagonalMatrix: torch.Tensor):\n",
    "        \n",
    "        print(f'in multply with mainMatrix.shape ={mainMatrix.shape} anddiagonalMatrix.shape = {diagonalMatrix.shape}')\n",
    "        result = mainMatrix* diagonalMatrix\n",
    "        print(f'in multply with result.shape ={result.shape}')\n",
    "        \n",
    "        return result\n",
    "'''\n",
    "    \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestParallelRunWithPyTorch_Convolution(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(1,1), stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(1,1), stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.average_pooling2d_ = torch.nn.AvgPool2d(kernel_size=(4,5) )\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=40, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_average_pooling2d_ = self.average_pooling2d_(y_conv2d_1)\n",
    "        y_multiply = y_conv2d * y_average_pooling2d_\n",
    "        y_flatten = self.flatten(y_multiply)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "\n",
    "\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "device = 'cuda'\n",
    "# lambdaL2Regularization = 0.00;\n",
    "model = TestParallelRunWithPyTorch_Convolution().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = momentum)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      x_2_3_4_5,\n",
    "      y_2_2, \n",
    "      lr = lr, \n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      numEpochs = 10, \n",
    "      momentum = momentum\n",
    "      )    \n",
    "\n",
    "assert abs(loss_before-0.6883244514465332) < 1e-6\n",
    "assert abs(loss_after-0.5018750429153442) < 1e-6\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
