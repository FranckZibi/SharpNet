{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8202f5",
   "metadata": {},
   "source": [
    "# Common Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e593dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the directory\n",
    "directory = os.path.abspath('../../Prod/Python/')\n",
    "\n",
    "# Add the directory to sys.path if not already there\n",
    "if directory not in sys.path:\n",
    "    sys.path.append(directory)\n",
    "\n",
    "from pytorch_utils import pytorch_utils\n",
    "from pytorch_to_sharpnet import pytorch_to_sharpnet\n",
    "Train = pytorch_utils.Train\n",
    "numpy_array_for_tests = pytorch_utils.numpy_array_for_tests\n",
    "y_numpy_array_for_tests = pytorch_utils.y_numpy_array_for_tests\n",
    "save_sharpnet = pytorch_to_sharpnet.save_sharpnet\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "print(f'log_file_name: {pytorch_utils.log_file_name}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "\n",
    "\n",
    "#to save a model in sharpnet format:\n",
    "#save_sharpnet(model, 'model_pytorch', os.getcwd(), optimizer, torch.nn.HuberLoss(delta=0.5), input_shape=[rows,3,2,2], verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a28a32",
   "metadata": {},
   "source": [
    "# Compute Gradient in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "input = torch.tensor( [[0.01995766907930374, 0.05039015784859657, 0.04157894104719162]], requires_grad=True)\n",
    "target = torch.tensor([[1.0, 0, 0]])\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "input.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094e90a",
   "metadata": {},
   "source": [
    "# Compute Gradient in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54221f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "# Define your predictions and ground truth\n",
    "predictions = tf.constant([[0.01995766907930374, 0.05039015784859657, 0.04157894104719162]])\n",
    "ground_truth = tf.constant([[1.0, 0, 0]])\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) loss\n",
    "mse_loss = tf.reduce_mean(tf.square(predictions - ground_truth))\n",
    "\n",
    "# Use GradientTape to compute the gradient\n",
    "with tf.GradientTape() as tape:\n",
    "    # Watch the variables we want to compute gradients with respect to\n",
    "    tape.watch(predictions)\n",
    "    tape.watch(ground_truth)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = tf.reduce_mean(tf.square(predictions - ground_truth))\n",
    "\n",
    "# Compute the gradient of the loss with respect to the predictions\n",
    "gradients = tape.gradient(loss, predictions)\n",
    "gradients\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68cb1f8",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestParallelRunWithPyTorch_Mse(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.dense = torch.nn.Linear(5, 3)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.flatten_dense_1 = torch.nn.Flatten(1)\n",
    "        self.dense_1 = torch.nn.Linear(3*4*3, 3)\n",
    "        torch.nn.init.zeros_(self.dense_1.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_dense = self.dense(x)\n",
    "        y_relu = self.relu(y_dense)\n",
    "        y_flatten_dense_1 = self.flatten_dense_1(y_relu)\n",
    "        y_dense_1 = self.dense_1(y_flatten_dense_1)\n",
    "        return y_dense_1\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_Mse().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 3, 4, 5]),\n",
    "      y_numpy_array_for_tests(2, 3),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.MSELoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10\n",
    "      )\n",
    "\n",
    "assert abs(loss_before-0.297953724861145) < 1e-6\n",
    "assert abs(loss_after-0.09050967544317245) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94358ccf",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Mse_AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "class TestParallelRunWithPyTorch_Mse_AdamW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*1*1, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_flatten_dense = self.flatten_dense(x)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        return y_dense\n",
    "\n",
    "model = TestParallelRunWithPyTorch_Mse_AdamW().to(device)\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_Mse_AdamW().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1.0, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.1)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([1, 1, 1, 1]),\n",
    "      y_numpy_array_for_tests(1, 2),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.MSELoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 1\n",
    "      )\n",
    "\n",
    "assert abs(loss_before-0.6364270448684692) < 1e-6\n",
    "assert abs(loss_after-1.6577095985412598) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f65f26",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567cf1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestParallelRunWithPyTorch_Convolution(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(1,1), stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(1,1), stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.average_pooling2d_ = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=40, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_average_pooling2d_ = self.average_pooling2d_(y_conv2d_1)\n",
    "        y_multiply = y_conv2d * y_average_pooling2d_\n",
    "        y_flatten = self.flatten(y_multiply)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_Convolution().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 3, 4, 5]),\n",
    "      y_numpy_array_for_tests(2, 2),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "assert abs(loss_before-0.6931346654891968) < 1e-6\n",
    "assert abs(loss_after-0.6924928426742554) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d677ace",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Convolution_AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069059b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestParallelRunWithPyTorch_Convolution_AdamW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(1,1), stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(1,1), stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.average_pooling2d_ = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=40, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_average_pooling2d_ = self.average_pooling2d_(y_conv2d_1)\n",
    "        y_multiply = y_conv2d * y_average_pooling2d_\n",
    "        y_flatten = self.flatten(y_multiply)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_Convolution_AdamW().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.005)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 3, 4, 5]),\n",
    "      y_numpy_array_for_tests(2, 2),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "\n",
    "assert abs(loss_before-0.6931346654891968) < 1e-6\n",
    "assert abs(loss_after-0.6222155690193176) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf1eaa",
   "metadata": {},
   "source": [
    "# Test_Convolution_With_Asymmetric_Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecc732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Test_Convolution_With_Asymmetric_Padding(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=1, kernel_size=(3,3), stride=2, padding=(1,1), bias=False)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=6, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_flatten = self.flatten(y_conv2d)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_Convolution_With_Asymmetric_Padding().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9, weight_decay=0.15)\n",
    "rows = 2 # fix to correct number of rows\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([rows, 3, 4, 5]),\n",
    "    y_numpy_array_for_tests(rows, 2),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-0.6809013485908508) < 1e-6\n",
    "#??D assert abs(loss_after-0.456775605678558354) < 1e-6\n",
    "assert abs(loss_after-0.456775605678558354) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ecae2",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestParallelRunWithPyTorch_Conv1D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv1d = torch.nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1d.bias)\n",
    "        self.conv1d_1 = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3, stride=2, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1d_1.bias)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv1d = self.conv1d(x)\n",
    "        y_conv1d_1 = self.conv1d_1(y_conv1d)\n",
    "        y_flatten = self.flatten(y_conv1d_1)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_Conv1D().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([3, 4, 5]),\n",
    "      y_numpy_array_for_tests(3, 3),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "assert abs(loss_before-1.0818352699279785) < 1e-6\n",
    "assert abs(loss_after-0.7389676570892334) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7419b83",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_LayerNormalizationNchw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TestParallelRunWithPyTorch_LayerNormalizationNchw(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(5,5), stride=2, padding=(2,2), bias=False)\n",
    "        self.layer_normalization = torch.nn.LayerNorm(normalized_shape=(3), eps=0.001)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=12, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_layer_normalization = self.layer_normalization(y_conv2d)\n",
    "        y_flatten = self.flatten(y_layer_normalization)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_LayerNormalizationNchw().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 3, 4, 5]),\n",
    "      y_numpy_array_for_tests(2, 2),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "assert abs(loss_before-0.8300741910934448) < 1e-6\n",
    "assert abs(loss_after-0.5169422626495361) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614038c5",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_RMSNormNchw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1652737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TestParallelRunWithPyTorch_RMSNormNchw(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(5,5), stride=2, padding=(2,2), bias=False)\n",
    "        self.rms_normalization = torch.nn.RMSNorm(normalized_shape=(3), eps = 0.001)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=12, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_rms_normalization = self.rms_normalization(y_conv2d)\n",
    "        y_flatten = self.flatten(y_rms_normalization)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_RMSNormNchw().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 3, 4, 5]),\n",
    "      y_numpy_array_for_tests(2, 2),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "assert abs(loss_before-0.8518396019935608) < 1e-6\n",
    "assert abs(loss_after-0.6556870937347412) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e7da0",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_BatchNormalizationNchw2345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TestParallelRunWithPyTorch_BatchNormalizationNchw2345(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.batch_normalization = torch.nn.BatchNorm2d(num_features=3, eps=0.001, momentum=0.010000000000000009)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_relu = self.relu(x)\n",
    "        y_batch_normalization = self.batch_normalization(y_relu)\n",
    "        y_flatten = self.flatten(y_batch_normalization)\n",
    "        return y_flatten\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_BatchNormalizationNchw2345().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([10, 3, 4, 5]),\n",
    "    y_numpy_array_for_tests(10, 60),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.L1Loss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-0.2588789165019989) < 1e-6\n",
    "assert abs(loss_after-0.035503968596458435) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f66d55",
   "metadata": {},
   "source": [
    "# TestResNet_Shortcut_Same_Dimension_NCHW_2_1_4_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestResNet_Shortcut_Same_Dimension_NCHW_2_1_4_4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*4*4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_add = y_conv2d_1 + y_conv2d\n",
    "        y_flatten_dense = self.flatten_dense(y_add)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        return y_dense\n",
    "    \n",
    "\n",
    "device = 'cuda'\n",
    "model = TestResNet_Shortcut_Same_Dimension_NCHW_2_1_4_4().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 1, 4, 4]),\n",
    "      y_numpy_array_for_tests(2, 3),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "assert abs(loss_before-1.0986199378967285) < 1e-6\n",
    "assert abs(loss_after-0.7018476724624634) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e3e33",
   "metadata": {},
   "source": [
    "# TestResNet_Shortcut_Different_Dimension_With_Conv_1x1_to_change_Dimension_NCHW_2_1_4_4()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestResNet_Shortcut_Different_Dimension_With_Conv_1x1_to_change_Dimension_NCHW_2_1_4_4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.conv2d_2 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_2.bias)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*4*4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_conv2d_2 = self.conv2d_2(y_conv2d)\n",
    "        y_add = y_conv2d_2 + y_conv2d_1\n",
    "        y_flatten_dense = self.flatten_dense(y_add)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        return y_dense\n",
    "    \n",
    "    \n",
    "device = 'cuda'\n",
    "# lambdaL2Regularization = 0.00;\n",
    "model = TestResNet_Shortcut_Different_Dimension_With_Conv_1x1_to_change_Dimension_NCHW_2_1_4_4().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 1, 4, 4]),\n",
    "      y_numpy_array_for_tests(2, 3),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10\n",
    "      )    \n",
    "    \n",
    "    \n",
    "assert abs(loss_before-1.0974769592285156) < 1e-6\n",
    "assert abs(loss_after-0.5784467458724976) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981cee7",
   "metadata": {},
   "source": [
    "# TestL2Regularization_ConvolutionLayer_SGDVanilla_NCHW_2_1_4_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestL2Regularization_ConvolutionLayer_SGDVanilla_NCHW_2_1_4_4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*4*4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_flatten_dense = self.flatten_dense(y_conv2d)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestL2Regularization_ConvolutionLayer_SGDVanilla_NCHW_2_1_4_4().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.6, weight_decay=0.9)\n",
    "rows = 2 # fix to correct number of rows\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([rows, 1, 4, 4]),\n",
    "    y_numpy_array_for_tests(rows, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10\n",
    "    )\n",
    "    \n",
    "assert abs(loss_before-1.099147081375122) < 1e-6\n",
    "assert abs(loss_after-0.9695441722869873) < 1e-6    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1d502",
   "metadata": {},
   "source": [
    "# TestReluActivation_NCHW_2_1_4_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestReluActivation_NCHW_2_1_4_4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*4*4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dense_1 = torch.nn.Linear(in_features=3, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense_1.bias)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_flatten_dense = self.flatten_dense(x)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        y_relu = self.relu(y_dense)\n",
    "        y_dense_1 = self.dense_1(y_relu)\n",
    "        y_sigmoid = self.sigmoid(y_dense_1)\n",
    "        return y_sigmoid\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestReluActivation_NCHW_2_1_4_4().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9, weight_decay=0.03)\n",
    "rows = 2 # fix to correct number of rows\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([rows, 1, 4, 4]),\n",
    "    y_numpy_array_for_tests(rows, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.BCELoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10\n",
    "    )\n",
    "\n",
    "    \n",
    "assert abs(loss_before-0.6726791262626648) < 1e-6\n",
    "assert abs(loss_after-0.5357139110565186) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8afb96",
   "metadata": {},
   "source": [
    "# Test_Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Test_Huber(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=3*2*2, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dense_1 = torch.nn.Linear(in_features=3, out_features=1, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense_1.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_flatten_dense = self.flatten_dense(x)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        y_relu = self.relu(y_dense)\n",
    "        y_dense_1 = self.dense_1(y_relu)\n",
    "        return y_dense_1\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_Huber().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9, weight_decay=0.05)\n",
    "rows = 4 # fix to correct number of rows\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([rows, 3, 2, 2]),\n",
    "    y_numpy_array_for_tests(rows, 1),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.HuberLoss(delta=0.5),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size=2\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-0.32030075788497925) < 1e-6\n",
    "assert abs(loss_after-0.0027381146792322397) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106029c",
   "metadata": {},
   "source": [
    "# Test_DepthwiseConvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3191aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Test_DepthwiseConvolution(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.depthwiseconv2d = torch.nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(3,3), stride=1, padding='same', groups=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.depthwiseconv2d.bias)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=8, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_depthwiseconv2d = self.depthwiseconv2d(x)\n",
    "        y_flatten = self.flatten(y_depthwiseconv2d)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_DepthwiseConvolution().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9, weight_decay=0.01)\n",
    "rows = 12 # fix to correct number of rows\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([rows, 2, 2, 2]),\n",
    "    y_numpy_array_for_tests(rows, 2),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size=4\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-0.700163722038269) < 1e-6\n",
    "assert abs(loss_after-0.6944987177848816) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41b40c",
   "metadata": {},
   "source": [
    "# TestConcatenate_NCHW_9_1_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c70758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestConcatenate_NCHW_9_1_1_1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', groups=1, bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', groups=1, bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=2, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_concatenate = torch.cat((y_conv2d, y_conv2d_1), dim = 1)\n",
    "        y_flatten = self.flatten(y_concatenate)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestConcatenate_NCHW_9_1_1_1().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.75, weight_decay=0.07, nesterov=True)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([9, 1, 1, 1]),\n",
    "    y_numpy_array_for_tests(9, 2),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 3\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-0.6930493116378784) < 1e-6\n",
    "assert abs(loss_after-0.6860392093658447) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355299c0",
   "metadata": {},
   "source": [
    "# TestLeakyReluActivation_NCHW_2_1_4_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestLeakyReluActivation_NCHW_10_1_4_4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*4*4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "        self.leakyrelu = torch.nn.LeakyReLU(negative_slope=0.1)\n",
    "        self.dense_1 = torch.nn.Linear(in_features=3, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense_1.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_flatten_dense = self.flatten_dense(x)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        y_leakyrelu = self.leakyrelu(y_dense)\n",
    "        y_dense_1 = self.dense_1(y_leakyrelu)\n",
    "        return y_dense_1\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestLeakyReluActivation_NCHW_10_1_4_4().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.66, weight_decay=0.17, nesterov=True)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([10, 1, 4, 4]),\n",
    "    y_numpy_array_for_tests(10, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "\n",
    "assert abs(loss_before-1.1183210611343384) < 1e-6\n",
    "assert abs(loss_after-1.0901305675506592) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e1b04",
   "metadata": {},
   "source": [
    "# TestMultiply_NCHW_2_3_4_5_different_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0030f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestMultiply_NCHW_2_3_4_5_different_dimension(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(1,1), stride=1, padding='same', groups=1, bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(1,1), stride=1, padding='same', groups=1, bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.max_pooling2d_ = torch.nn.MaxPool2d(kernel_size=(4,5) )\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=40, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_max_pooling2d_ = self.max_pooling2d_(y_conv2d_1)\n",
    "        y_multiply = y_conv2d * y_max_pooling2d_\n",
    "        y_flatten = self.flatten(y_multiply)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestMultiply_NCHW_2_3_4_5_different_dimension().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9, weight_decay=0.07)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([2, 3, 4, 5]),\n",
    "    y_numpy_array_for_tests(2, 2),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "assert abs(loss_before-0.6887038946151733) < 1e-6\n",
    "assert abs(loss_after-0.6871695518493652) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b34fd8",
   "metadata": {},
   "source": [
    "# Test_SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Test_SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.bidirectional = torch.nn.RNN(input_size=1, hidden_size=2, num_layers=2, nonlinearity='tanh', bias=True, batch_first=True, dropout=0, bidirectional=True)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l0)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l0)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l1)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l1)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l1_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l1_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l0_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l0_reverse)\n",
    "        self.simple_rnn_1 = torch.nn.RNN(input_size=4, hidden_size=2, num_layers=1, nonlinearity='tanh', bias=True, batch_first=True, dropout=0, bidirectional=False)\n",
    "        torch.nn.init.zeros_(self.simple_rnn_1.bias_ih_l0)\n",
    "        torch.nn.init.zeros_(self.simple_rnn_1.bias_hh_l0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_bidirectional, y_bidirectional_hidden = self.bidirectional(x)\n",
    "        y_simple_rnn_1_return_sequences, y_simple_rnn_1_hidden_return_sequences = self.simple_rnn_1(y_bidirectional)\n",
    "        y_simple_rnn_1 = y_simple_rnn_1_return_sequences[:,-1,:]\n",
    "        return y_simple_rnn_1\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_SimpleRNN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([4, 2, 1]),\n",
    "    y_numpy_array_for_tests(4, 2),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.MSELoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "\n",
    "assert abs(loss_before-0.48370790481567383) < 1e-6\n",
    "assert abs(loss_after-0.2722862660884857) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3166277b",
   "metadata": {},
   "source": [
    "# Test_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Test_LSTM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.bidirectional = torch.nn.LSTM(input_size=1, hidden_size=1, num_layers=2, bias=True, batch_first=True, dropout=0.25, bidirectional=True)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l0)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l0)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l1)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l1)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l1_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l1_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l0_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l0_reverse)\n",
    "        self.lstm_1 = torch.nn.LSTM(input_size=2, hidden_size=1, num_layers=1, bias=True, batch_first=True, dropout=0, bidirectional=False)\n",
    "        torch.nn.init.zeros_(self.lstm_1.bias_ih_l0)\n",
    "        torch.nn.init.zeros_(self.lstm_1.bias_hh_l0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_bidirectional, y_bidirectional_hidden = self.bidirectional(x)\n",
    "        y_lstm_1_return_sequences, y_lstm_1_hidden_return_sequences = self.lstm_1(y_bidirectional)\n",
    "        y_lstm_1 = y_lstm_1_return_sequences[:,-1,:]\n",
    "        return y_lstm_1\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_LSTM().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([4, 2, 1]),\n",
    "    y_numpy_array_for_tests(4, 1),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.MSELoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 1,\n",
    "    batch_size = 2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b287723",
   "metadata": {},
   "source": [
    "# Test_DotProductAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e124a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Test_DotProductAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv1D_Q = torch.nn.Conv1d(in_channels=4, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_Q.bias)\n",
    "        self.conv1D_K = torch.nn.Conv1d(in_channels=4, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_K.bias)\n",
    "        self.conv1D_V = torch.nn.Conv1d(in_channels=4, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_V.bias)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=10, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv1D_Q = self.conv1D_Q(x)\n",
    "        y_conv1D_K = self.conv1D_K(x)\n",
    "        y_conv1D_V = self.conv1D_V(x)\n",
    "        y_scaleddotproductattention = F.scaled_dot_product_attention(y_conv1D_Q, y_conv1D_K, y_conv1D_V, attn_mask=None, dropout_p=0.0, is_causal=False, scale=1)\n",
    "        y_flatten = self.flatten(y_scaleddotproductattention)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_DotProductAttention().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9, weight_decay=0.07)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([6, 4, 5]),\n",
    "    y_numpy_array_for_tests(6, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-1.0957681735356648) < 1e-6\n",
    "assert abs(loss_after-1.0050253868103027) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973468b",
   "metadata": {},
   "source": [
    "# Test_MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d827d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Test_MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv1D_Q = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_Q.bias)\n",
    "        self.conv1D_K = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_K.bias)\n",
    "        self.conv1D_V = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_V.bias)\n",
    "        self.attn_mask_dict = dict()\n",
    "        self.multi_head_attention = torch.nn.MultiheadAttention(embed_dim=6, num_heads=3, dropout=0, bias=False, add_bias_kv=False, add_zero_attn=False, batch_first=True)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=12, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv1D_Q = self.conv1D_Q(x)\n",
    "        y_conv1D_K = self.conv1D_K(x)\n",
    "        y_conv1D_V = self.conv1D_V(x)\n",
    "        attn_mask = None\n",
    "        \n",
    "        # We build the attention mask 'attn_mask'\n",
    "        if not isinstance(y_conv1D_Q, torch.fx.proxy.Proxy):\n",
    "            sz = y_conv1D_Q.size(1)  # L: Target sequence Length\n",
    "            if sz not in self.attn_mask_dict: self.attn_mask_dict[sz] = torch.nn.Transformer.generate_square_subsequent_mask(sz, device = x.device, dtype = x.dtype)\n",
    "            attn_mask = self.attn_mask_dict[sz]\n",
    "        \n",
    "        y_multi_head_attention, _ = self.multi_head_attention(y_conv1D_Q, y_conv1D_K, y_conv1D_V, key_padding_mask=None, need_weights=False, attn_mask=attn_mask, average_attn_weights=False, is_causal=True)\n",
    "        y_flatten = self.flatten(y_multi_head_attention)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_MultiHeadAttention().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([10, 2, 6]),\n",
    "    y_numpy_array_for_tests(10, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-1.103563904762268) < 1e-6\n",
    "assert abs(loss_after-1.0815070867538452) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b26f481",
   "metadata": {},
   "source": [
    "# Test_MultiHeadAttention_with_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01926011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Test_MultiHeadAttention_with_bias(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv1D_Q = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_Q.bias)\n",
    "        self.conv1D_K = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_K.bias)\n",
    "        self.conv1D_V = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_V.bias)\n",
    "        self.attn_mask_dict = dict()\n",
    "        self.multi_head_attention = torch.nn.MultiheadAttention(embed_dim=6, num_heads=3, dropout=0, bias=True, add_bias_kv=False, add_zero_attn=False, batch_first=True)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=12, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv1D_Q = self.conv1D_Q(x)\n",
    "        y_conv1D_K = self.conv1D_K(x)\n",
    "        y_conv1D_V = self.conv1D_V(x)\n",
    "        attn_mask = None\n",
    "        \n",
    "        # We build the attention mask 'attn_mask'\n",
    "        if not isinstance(y_conv1D_Q, torch.fx.proxy.Proxy):\n",
    "            sz = y_conv1D_Q.size(1)  # L: Target sequence Length\n",
    "            if sz not in self.attn_mask_dict: self.attn_mask_dict[sz] = torch.nn.Transformer.generate_square_subsequent_mask(sz, device = x.device, dtype = x.dtype)\n",
    "            attn_mask = self.attn_mask_dict[sz]\n",
    "        \n",
    "        y_multi_head_attention, _ = self.multi_head_attention(y_conv1D_Q, y_conv1D_K, y_conv1D_V, key_padding_mask=None, need_weights=False, attn_mask=attn_mask, average_attn_weights=False, is_causal=True)\n",
    "        y_flatten = self.flatten(y_multi_head_attention)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_MultiHeadAttention_with_bias().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([10, 2, 6]),\n",
    "    y_numpy_array_for_tests(10, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "\n",
    "assert abs(loss_before-1.1090373992919922) < 1e-6\n",
    "assert abs(loss_after-1.0816659927368164) < 1e-6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
