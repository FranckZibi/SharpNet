{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8202f5",
   "metadata": {},
   "source": [
    "# Common Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d009d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchinfo import summary\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "log_file_name = str( Path(os.path.abspath(''))  /  \"PyTorch.log\" )\n",
    "print(f'log_file_name: {log_file_name}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "\n",
    "\n",
    "def numpy_array_for_tests(shape) -> np.array:\n",
    "    count = math.prod(shape)\n",
    "    return np.array([  2*(i+1) /count*((i%2)-0.5) for i in range(count)] ).reshape(shape)\n",
    "    \n",
    "def y_numpy_array_for_tests(rows, cols) -> np.array:\n",
    "    data = [0.0]*rows*cols\n",
    "    for row in range(rows):\n",
    "        data[row*cols+ row%cols ] = 1.0\n",
    "    return np.array(data).reshape([rows, cols])\n",
    "\n",
    "def truncate(s, max_size):\n",
    "    if len(s)<=max_size:\n",
    "        return s\n",
    "    return s[:(max_size//2)]+\"...\"+s[-(max_size//2):]\n",
    "\n",
    "def log(tensor, tensor_name = \"\", display_to_screen = True):\n",
    "    with torch.no_grad():\n",
    "        if isinstance(tensor, torch.nn.parameter.Parameter) or isinstance(tensor, torch.Tensor):\n",
    "            return log(tensor.detach().to('cpu').numpy(), tensor_name, display_to_screen)\n",
    "        if isinstance(tensor, list):\n",
    "            for e in tensor:\n",
    "                log(e, tensor_name, display_to_screen)\n",
    "            return\n",
    "        asString = None\n",
    "        if isinstance(tensor,str):\n",
    "            asString = tensor\n",
    "        elif isinstance(tensor,np.ndarray):\n",
    "            asString = str(tensor_name) + \" \" + str(tensor.shape) + \"\\n\" + truncate(str(tensor.tolist()), 10000)\n",
    "        else:\n",
    "            asString = str(tensor)\n",
    "        with open(log_file_name, \"a\") as myfile:\n",
    "            myfile.write(asString + \"\\n\")\n",
    "        if display_to_screen:\n",
    "            print(asString)\n",
    "\n",
    "def print_intermediate_layer_model(layer, layer_name, layer_input: torch.Tensor, display_to_screen) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            if layer_input is None:\n",
    "                return None\n",
    "            if not isinstance(layer, torch.nn.Module):\n",
    "                raise Exception(f'not a layer {type(layer)}  {layer}')            \n",
    "            layer_output = layer.forward(layer_input)\n",
    "            log(layer_output, layer_name, display_to_screen)\n",
    "            return layer_output\n",
    "        except Exception as e:  \n",
    "            #print(e)\n",
    "            return None\n",
    "        \n",
    "\n",
    "def print_weights(model: torch.nn.Module) -> None:\n",
    "    import torchvision\n",
    "    m = torchvision.models.efficientnet_b0()\n",
    "    a= dict(m.named_parameters())\n",
    "    total_weight = 0\n",
    "    for key,val in a.items():\n",
    "        key_count = torch.numel(val.data) \n",
    "        log(key+\":\"+str(key_count))\n",
    "        total_weight += key_count\n",
    "    log('total_weight: '+str(total_weight))         \n",
    "        \n",
    "def print_model(model, inputs, display_weights = True, display_layers_output = False, display_layers_grads = False, display_to_screen = False):\n",
    "    if not isinstance(model, torch.nn.Module):\n",
    "        raise Exception(f'not a model{type(model)}  {model}')            \n",
    "    with torch.no_grad():\n",
    "        layers = list(model.named_modules())[1:]\n",
    "\n",
    "        layer_input = inputs\n",
    "        for layerIndex in range(len(layers)):\n",
    "            if layerIndex>3 and layerIndex <  (len(layers)-3 and False)  :\n",
    "                continue\n",
    "            layer_name = layers[layerIndex][0]\n",
    "            layer = layers[layerIndex][1]\n",
    "            log(\"--------------------------------------------------------------------\\n-\",\"\", display_to_screen)           \n",
    "            log(\"layer#\"+ str(layerIndex)+\": \"+layer_name, \"\", display_to_screen)\n",
    "            #log(str(layer.get_config()), \"get_config:\", False)\n",
    "            if display_layers_grads:\n",
    "                for p_name, p in layer.named_parameters():\n",
    "                    if p.grad is not None:\n",
    "                        log(p.grad, layer_name+\".\"+p_name+\" grad:\", display_to_screen)\n",
    "            if display_weights:\n",
    "                for p_name, p in layer.named_parameters():\n",
    "                    log(p, layer_name+\".\"+p_name, display_to_screen)\n",
    "                if hasattr(layer, 'running_mean'):\n",
    "                    log(layer.running_mean, layer_name+\".running_mean\", display_to_screen)\n",
    "                if hasattr(layer, 'running_var'):\n",
    "                    log(layer.running_var, layer_name+\".running_var\", display_to_screen)\n",
    "                    \n",
    "            log(\"-\", \"\", display_to_screen)\n",
    "            if display_layers_output:\n",
    "                log(\"layer#\"+ str(layerIndex)+\" output:\", \"\", display_to_screen)\n",
    "                layer_input = print_intermediate_layer_model(layer, layer_name, layer_input, display_to_screen)\n",
    "        log(\"--------------------------------------------------------------------\", display_to_screen=display_to_screen)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "def Train(model: torch.nn.Module, \n",
    "          X_numpy: np.array,\n",
    "          Y_numpy: np.array,\n",
    "          device: str,\n",
    "          loss_criterion,\n",
    "          optimizer,\n",
    "          num_epochs: int,\n",
    "          batch_size: int|None = None,\n",
    "          verbose: bool = True\n",
    "         ) -> (float, float):\n",
    "\n",
    "    # by default, the batch size will proceed all elements at once\n",
    "    \n",
    "    if not batch_size or batch_size <= 0:\n",
    "        batch_size = X_numpy.shape[0]\n",
    "    \n",
    "    if os.path.isfile(log_file_name):\n",
    "        try:\n",
    "            os.remove(log_file_name)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    X = torch.tensor(X_numpy, device=device, dtype=torch.float32)\n",
    "    Y_true = torch.tensor(Y_numpy, device=device, dtype=torch.float32)\n",
    "    dataset = TensorDataset(X, Y_true) \n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    log( \"--------------------------------------------------------------------\\n-\")\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        try:\n",
    "            pass #summary(model, input_size=(X.shape[1:]), device = device)\n",
    "        except Exception as e:\n",
    "            log(f'fail to create model summay: {e}')\n",
    "    log(str(model))\n",
    "            \n",
    "    model_summary = f.getvalue()\n",
    "    log(str(model_summary), '', True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        Y_pred_before = model.forward(X)\n",
    "        train_loss_before = loss_criterion(Y_pred_before, Y_true).to('cpu').item()\n",
    "        model.train()\n",
    "\n",
    "    log( \"-\\n--------------------------------------------------------------------\\n-\", \"\", False)\n",
    "    log(X,\"X\", False)\n",
    "    log(Y_true,\"Y_true\", False)\n",
    "    log(\"--------------------------------------------------------------------\\nmodel before trainig\",\"\", False)\n",
    "    #log( \"--------------------------------------------------------------------\\nprint_layers(model) before\\n-\")\n",
    "    if verbose:\n",
    "        print_model(model, X, display_weights = True, display_layers_output = True, display_to_screen = False)\n",
    "\n",
    "\n",
    "\n",
    "    print( \"-\")\n",
    "    print( \"--------------------------------------------------------------------\")\n",
    "    print( \"-\")\n",
    "    print( \"training for \",num_epochs,\" epochs...\")\n",
    "\n",
    "    log( \"-\\n--------------------------------------------------------------------\\n-\\nmodel after trainig\")\n",
    "    # log( \"--------------------------------------------------------------------\\nprint_layers(model) after\\n-\")\n",
    "\n",
    "\n",
    "    for epoch in range(0,num_epochs):\n",
    "        log(f'Epoch#{epoch}', '', False)\n",
    "        for batch_id, (x_batch,y_batch_true) in enumerate(data_loader):\n",
    "            if verbose:\n",
    "                log(f'Epoch#{epoch} / batch_id#{batch_id}', '', False)\n",
    "            y_batch_pred = model(x_batch)\n",
    "            l = loss_criterion(y_batch_pred, y_batch_true) # compute loss\n",
    "            #with torch.no_grad():\n",
    "            #    log(f'loss for epoch#{epoch} / batch_id#{batch_id} : {l.cpu().item()}', '', True)\n",
    "            l.backward() # compute gradients\n",
    "            optimizer.step() # update weights\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if verbose:\n",
    "                    print_model(model, X, display_weights = True, display_layers_grads = True ,display_layers_output = False, display_to_screen = False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        Y_pref_after = model.forward(X)\n",
    "        model.train()\n",
    "        train_loss_after = loss_criterion(Y_pref_after, Y_true).to('cpu').item()\n",
    "\n",
    "    if verbose:\n",
    "        print_model(model, X, display_weights = True, display_layers_output = True, display_to_screen = False)\n",
    "    log(\"PyTorch num_epochs = \" + str(num_epochs))\n",
    "    # log(\"TF learningRate = \"+str(keras.backend.eval(optim.lr)))\n",
    "    if 'lr' in optimizer.param_groups[0]:    \n",
    "        log(\"PyTorch lr = \" + str(optimizer.param_groups[0]['lr']))\n",
    "    if 'momentum' in optimizer.param_groups[0]:    \n",
    "        log(\"PyTorch momentum = \" + str(optimizer.param_groups[0]['momentum']))\n",
    "    if 'nesterov' in optimizer.param_groups[0] and optimizer.param_groups[0]['nesterov']:\n",
    "        log(\"PyTorch nesterov = \" + str(optimizer.param_groups[0]['nesterov']))\n",
    "    log(\"PyTorch batch_size = \" + str(batch_size))\n",
    "    if 'weight_decay' in optimizer.param_groups[0]:    \n",
    "        log(\"PyTorch lambdaL2Regularization = \" + str(optimizer.param_groups[0]['weight_decay']))\n",
    "    #log(\"PyTorch use_bias = \" + str(use_bias))\n",
    "    log(Y_pred_before, \"PyTorch Y_pred_before\", True)\n",
    "    log(\"PyTorch loss_before=\"+str(train_loss_before))\n",
    "    log(Y_pref_after, \"PyTorch Y_pref_after\", True)\n",
    "    log(\"PyTorch loss_after=\"+str(train_loss_after))\n",
    "    return train_loss_before, train_loss_after\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 50\n",
    "import logging\n",
    "logger = logging.getLogger('tt')\n",
    "\n",
    "try:\n",
    "    a = min(a, 10/0)\n",
    "    print('ok', a)    \n",
    "except Exception as exc:\n",
    "    logger.warning(\n",
    "        f\"Fail to retrieve tokenizer max length: {exc}\", exc_info=True\n",
    "    )\n",
    "    #print(exc)\n",
    "    #print('exception', a)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a28a32",
   "metadata": {},
   "source": [
    "# Compute Gradient in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.MSELoss()\n",
    "input = torch.tensor( [[0.01995766907930374, 0.05039015784859657, 0.04157894104719162]], requires_grad=True)\n",
    "target = torch.tensor([[1.0, 0, 0]])\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "input.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094e90a",
   "metadata": {},
   "source": [
    "# Compute Gradient in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54221f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Define your predictions and ground truth\n",
    "predictions = tf.constant([[0.01995766907930374, 0.05039015784859657, 0.04157894104719162]])\n",
    "ground_truth = tf.constant([[1.0, 0, 0]])\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) loss\n",
    "mse_loss = tf.reduce_mean(tf.square(predictions - ground_truth))\n",
    "\n",
    "# Use GradientTape to compute the gradient\n",
    "with tf.GradientTape() as tape:\n",
    "    # Watch the variables we want to compute gradients with respect to\n",
    "    tape.watch(predictions)\n",
    "    tape.watch(ground_truth)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = tf.reduce_mean(tf.square(predictions - ground_truth))\n",
    "\n",
    "# Compute the gradient of the loss with respect to the predictions\n",
    "gradients = tape.gradient(loss, predictions)\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68cb1f8",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestParallelRunWithPyTorch_Mse(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.dense = torch.nn.Linear(5, 3)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.flatten_dense_1 = torch.nn.Flatten(1)\n",
    "        self.dense_1 = torch.nn.Linear(3*4*3, 3)\n",
    "        torch.nn.init.zeros_(self.dense_1.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_dense = self.dense(x)\n",
    "        y_relu = self.relu(y_dense)\n",
    "        y_flatten_dense_1 = self.flatten_dense_1(y_relu)\n",
    "        y_dense_1 = self.dense_1(y_flatten_dense_1)\n",
    "        return y_dense_1\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_Mse().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 3, 4, 5]),\n",
    "      y_numpy_array_for_tests(2, 3),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.MSELoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10\n",
    "      )\n",
    "\n",
    "assert abs(loss_before-0.297953724861145) < 1e-6\n",
    "assert abs(loss_after-0.09050967544317245) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94358ccf",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Mse_AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "class TestParallelRunWithPyTorch_Mse_AdamW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*1*1, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_flatten_dense = self.flatten_dense(x)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        return y_dense\n",
    "\n",
    "model = TestParallelRunWithPyTorch_Mse_AdamW().to(device)\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_Mse_AdamW().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1.0, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.1)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([1, 1, 1, 1]),\n",
    "      y_numpy_array_for_tests(1, 2),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.MSELoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 1\n",
    "      )\n",
    "\n",
    "assert abs(loss_before-0.6364270448684692) < 1e-6\n",
    "assert abs(loss_after-1.6577095985412598) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f65f26",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567cf1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestParallelRunWithPyTorch_Convolution(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(1,1), stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(1,1), stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.average_pooling2d_ = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=40, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_average_pooling2d_ = self.average_pooling2d_(y_conv2d_1)\n",
    "        y_multiply = y_conv2d * y_average_pooling2d_\n",
    "        y_flatten = self.flatten(y_multiply)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_Convolution().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 3, 4, 5]),\n",
    "      y_numpy_array_for_tests(2, 2),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "assert abs(loss_before-0.6931346654891968) < 1e-6\n",
    "assert abs(loss_after-0.6924928426742554) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d677ace",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Convolution_AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069059b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestParallelRunWithPyTorch_Convolution_AdamW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(1,1), stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(1,1), stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.average_pooling2d_ = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=40, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_average_pooling2d_ = self.average_pooling2d_(y_conv2d_1)\n",
    "        y_multiply = y_conv2d * y_average_pooling2d_\n",
    "        y_flatten = self.flatten(y_multiply)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_Convolution_AdamW().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.005)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 3, 4, 5]),\n",
    "      y_numpy_array_for_tests(2, 2),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "\n",
    "assert abs(loss_before-0.6931346654891968) < 1e-6\n",
    "assert abs(loss_after-0.6222155690193176) < 1e-6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf1eaa",
   "metadata": {},
   "source": [
    "# Test_Convolution_With_Asymmetric_Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecc732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Test_Convolution_With_Asymmetric_Padding(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=1, kernel_size=(3,3), stride=2, padding=(1,1), bias=False)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=6, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_flatten = self.flatten(y_conv2d)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_Convolution_With_Asymmetric_Padding().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9, weight_decay=0.15)\n",
    "rows = 2 # fix to correct number of rows\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([rows, 3, 4, 5]),\n",
    "    y_numpy_array_for_tests(rows, 2),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-0.6809013485908508) < 1e-6\n",
    "assert abs(loss_after-0.456775605678558354) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ecae2",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestParallelRunWithPyTorch_Conv1D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv1d = torch.nn.Conv1d(in_channels=4, out_channels=2, kernel_size=3, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1d.bias)\n",
    "        self.conv1d_1 = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=3, stride=2, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1d_1.bias)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv1d = self.conv1d(x)\n",
    "        y_conv1d_1 = self.conv1d_1(y_conv1d)\n",
    "        y_flatten = self.flatten(y_conv1d_1)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_Conv1D().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([3, 4, 5]),\n",
    "      y_numpy_array_for_tests(3, 3),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "assert abs(loss_before-1.0818352699279785) < 1e-6\n",
    "assert abs(loss_after-0.7389676570892334) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7419b83",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_LayerNormalizationNchw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TestParallelRunWithPyTorch_LayerNormalizationNchw(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(5,5), stride=2, padding=(2,2), bias=False)\n",
    "        self.layer_normalization = torch.nn.LayerNorm(normalized_shape=(3), eps=0.001)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=12, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_layer_normalization = self.layer_normalization(y_conv2d)\n",
    "        y_flatten = self.flatten(y_layer_normalization)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_LayerNormalizationNchw().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 3, 4, 5]),\n",
    "      y_numpy_array_for_tests(2, 2),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "assert abs(loss_before-0.8300741910934448) < 1e-6\n",
    "assert abs(loss_after-0.5169422626495361) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614038c5",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_RMSNormNchw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1652737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TestParallelRunWithPyTorch_RMSNormNchw(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(5,5), stride=2, padding=(2,2), bias=False)\n",
    "        self.rms_normalization = torch.nn.RMSNorm(normalized_shape=(3), eps = 0.001)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=12, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_rms_normalization = self.rms_normalization(y_conv2d)\n",
    "        y_flatten = self.flatten(y_rms_normalization)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_RMSNormNchw().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 3, 4, 5]),\n",
    "      y_numpy_array_for_tests(2, 2),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "assert abs(loss_before-0.8518396019935608) < 1e-6\n",
    "assert abs(loss_after-0.6556870937347412) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e7da0",
   "metadata": {},
   "source": [
    "# TestParallelRunWithPyTorch_BatchNormalizationNchw2345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TestParallelRunWithPyTorch_BatchNormalizationNchw2345(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.batch_normalization = torch.nn.BatchNorm2d(num_features=3, eps=0.001, momentum=0.010000000000000009)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_relu = self.relu(x)\n",
    "        y_batch_normalization = self.batch_normalization(y_relu)\n",
    "        y_flatten = self.flatten(y_batch_normalization)\n",
    "        return y_flatten\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_BatchNormalizationNchw2345().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([10, 3, 4, 5]),\n",
    "    y_numpy_array_for_tests(10, 60),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.L1Loss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-0.25887892544269564) < 1e-6\n",
    "assert abs(loss_after-0.03555814176797867) < 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "x_numpy = numpy_array_for_tests([1, 1, 1, 2])\n",
    "x = torch.tensor(x_numpy, device=device, dtype=torch.float32)\n",
    "x.fill_(0) \n",
    "\n",
    "batch_normalization = torch.nn.BatchNorm2d(num_features=1, eps=0.001, momentum=0.9, device=device, affine=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('x_numpy', x_numpy)\n",
    "    print('weight', batch_normalization.weight)\n",
    "    print('bias', batch_normalization.bias)\n",
    "    print('running_mean', batch_normalization.running_mean)\n",
    "    print('running_var', batch_normalization.running_var)\n",
    "\n",
    "for i in range(0,10):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_normalization.training = False\n",
    "        y_inference = batch_normalization(x)\n",
    "        print('y_inference', y_inference)\n",
    "\n",
    "        batch_normalization.training = True\n",
    "        y_training = batch_normalization(x)\n",
    "        print('y_training', y_training)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print('x_numpy', x_numpy)\n",
    "    print('weight', batch_normalization.weight)\n",
    "    print('bias', batch_normalization.bias)\n",
    "    print('running_mean', batch_normalization.running_mean)\n",
    "    print('running_var', batch_normalization.running_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a6b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestParallelRunWithPyTorch_BatchNormalizationNchw2345(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(5,5), stride=1, padding='same', groups=1, bias=False)\n",
    "        self.batch_normalization = torch.nn.BatchNorm2d(num_features=2, eps=0.001, momentum=0.99)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=4, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_batch_normalization = self.batch_normalization(y_conv2d)\n",
    "        y_flatten = self.flatten(y_batch_normalization)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestParallelRunWithPyTorch_BatchNormalizationNchw2345().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1, momentum=0.9)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([1, 2, 1, 2]),\n",
    "    y_numpy_array_for_tests(1, 2),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 1,\n",
    "    batch_size = 1\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.training)\n",
    "model.eval()\n",
    "print(model.training)\n",
    "model.train()\n",
    "print(model.training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f66d55",
   "metadata": {},
   "source": [
    "# TestResNet_Shortcut_Same_Dimension_NCHW_2_1_4_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestResNet_Shortcut_Same_Dimension_NCHW_2_1_4_4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*4*4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_add = y_conv2d_1 + y_conv2d\n",
    "        y_flatten_dense = self.flatten_dense(y_add)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        return y_dense\n",
    "    \n",
    "\n",
    "device = 'cuda'\n",
    "model = TestResNet_Shortcut_Same_Dimension_NCHW_2_1_4_4().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 1, 4, 4]),\n",
    "      y_numpy_array_for_tests(2, 3),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10, \n",
    "      )    \n",
    "\n",
    "assert abs(loss_before-1.0986199378967285) < 1e-6\n",
    "assert abs(loss_after-0.7018476724624634) < 1e-6\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e3e33",
   "metadata": {},
   "source": [
    "# TestResNet_Shortcut_Different_Dimension_With_Conv_1x1_to_change_Dimension_NCHW_2_1_4_4()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestResNet_Shortcut_Different_Dimension_With_Conv_1x1_to_change_Dimension_NCHW_2_1_4_4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.conv2d_2 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_2.bias)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*4*4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_conv2d_2 = self.conv2d_2(y_conv2d)\n",
    "        y_add = y_conv2d_2 + y_conv2d_1\n",
    "        y_flatten_dense = self.flatten_dense(y_add)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        return y_dense\n",
    "    \n",
    "    \n",
    "device = 'cuda'\n",
    "# lambdaL2Regularization = 0.00;\n",
    "model = TestResNet_Shortcut_Different_Dimension_With_Conv_1x1_to_change_Dimension_NCHW_2_1_4_4().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9)\n",
    "\n",
    "loss_before, loss_after = Train(model, \n",
    "      numpy_array_for_tests([2, 1, 4, 4]),\n",
    "      y_numpy_array_for_tests(2, 3),\n",
    "      device = device,\n",
    "      loss_criterion = torch.nn.CrossEntropyLoss(), \n",
    "      optimizer = optimizer, \n",
    "      num_epochs = 10\n",
    "      )    \n",
    "    \n",
    "    \n",
    "assert abs(loss_before-1.0974769592285156) < 1e-6\n",
    "assert abs(loss_after-0.5784467458724976) < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981cee7",
   "metadata": {},
   "source": [
    "# TestL2Regularization_ConvolutionLayer_SGDVanilla_NCHW_2_1_4_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestL2Regularization_ConvolutionLayer_SGDVanilla_NCHW_2_1_4_4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*4*4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_flatten_dense = self.flatten_dense(y_conv2d)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestL2Regularization_ConvolutionLayer_SGDVanilla_NCHW_2_1_4_4().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.6, weight_decay=0.9)\n",
    "rows = 2 # fix to correct number of rows\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([rows, 1, 4, 4]),\n",
    "    y_numpy_array_for_tests(rows, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10\n",
    "    )\n",
    "    \n",
    "assert abs(loss_before-1.099147081375122) < 1e-6\n",
    "assert abs(loss_after-0.9695441722869873) < 1e-6    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1d502",
   "metadata": {},
   "source": [
    "# TestReluActivation_NCHW_2_1_4_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestReluActivation_NCHW_2_1_4_4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*4*4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dense_1 = torch.nn.Linear(in_features=3, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense_1.bias)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_flatten_dense = self.flatten_dense(x)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        y_relu = self.relu(y_dense)\n",
    "        y_dense_1 = self.dense_1(y_relu)\n",
    "        y_sigmoid = self.sigmoid(y_dense_1)\n",
    "        return y_sigmoid\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestReluActivation_NCHW_2_1_4_4().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9, weight_decay=0.03)\n",
    "rows = 2 # fix to correct number of rows\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([rows, 1, 4, 4]),\n",
    "    y_numpy_array_for_tests(rows, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.BCELoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10\n",
    "    )\n",
    "\n",
    "    \n",
    "assert abs(loss_before-0.6726791262626648) < 1e-6\n",
    "assert abs(loss_after-0.5357139110565186) < 1e-6    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8afb96",
   "metadata": {},
   "source": [
    "# Test_Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Test_Huber(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=3*2*2, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dense_1 = torch.nn.Linear(in_features=3, out_features=1, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense_1.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_flatten_dense = self.flatten_dense(x)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        y_relu = self.relu(y_dense)\n",
    "        y_dense_1 = self.dense_1(y_relu)\n",
    "        return y_dense_1\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_Huber().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9, weight_decay=0.05)\n",
    "rows = 4 # fix to correct number of rows\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([rows, 3, 2, 2]),\n",
    "    y_numpy_array_for_tests(rows, 1),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.HuberLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size=2\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-0.3986191749572754) < 1e-6\n",
    "assert abs(loss_after-0.0003985593211837113) < 1e-6    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106029c",
   "metadata": {},
   "source": [
    "# Test_DepthwiseConvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3191aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Test_DepthwiseConvolution(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.depthwiseconv2d = torch.nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(3,3), stride=1, padding='same', groups=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.depthwiseconv2d.bias)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=8, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_depthwiseconv2d = self.depthwiseconv2d(x)\n",
    "        y_flatten = self.flatten(y_depthwiseconv2d)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_DepthwiseConvolution().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9, weight_decay=0.01)\n",
    "rows = 12 # fix to correct number of rows\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([rows, 2, 2, 2]),\n",
    "    y_numpy_array_for_tests(rows, 2),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size=4\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-0.700163722038269) < 1e-6\n",
    "assert abs(loss_after-0.6944987177848816) < 1e-6    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41b40c",
   "metadata": {},
   "source": [
    "# TestConcatenate_NCHW_9_1_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c70758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestConcatenate_NCHW_9_1_1_1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', groups=1, bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,1), stride=1, padding='valid', groups=1, bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=2, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_concatenate = torch.cat((y_conv2d, y_conv2d_1), dim = 1)\n",
    "        y_flatten = self.flatten(y_concatenate)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestConcatenate_NCHW_9_1_1_1().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.75, weight_decay=0.07, nesterov=True)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([9, 1, 1, 1]),\n",
    "    y_numpy_array_for_tests(9, 2),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 3\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-0.6930493116378784) < 1e-6\n",
    "assert abs(loss_after-0.6860392093658447) < 1e-6    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355299c0",
   "metadata": {},
   "source": [
    "# TestLeakyReluActivation_NCHW_2_1_4_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestLeakyReluActivation_NCHW_10_1_4_4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.flatten_dense = torch.nn.Flatten(1)\n",
    "        self.dense = torch.nn.Linear(in_features=1*4*4, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "        self.leakyrelu = torch.nn.LeakyReLU(negative_slope=0.1)\n",
    "        self.dense_1 = torch.nn.Linear(in_features=3, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense_1.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_flatten_dense = self.flatten_dense(x)\n",
    "        y_dense = self.dense(y_flatten_dense)\n",
    "        y_leakyrelu = self.leakyrelu(y_dense)\n",
    "        y_dense_1 = self.dense_1(y_leakyrelu)\n",
    "        return y_dense_1\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestLeakyReluActivation_NCHW_10_1_4_4().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.66, weight_decay=0.17, nesterov=True)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([10, 1, 4, 4]),\n",
    "    y_numpy_array_for_tests(10, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "\n",
    "assert abs(loss_before-1.1183210611343384) < 1e-6\n",
    "assert abs(loss_after-1.0901305675506592) < 1e-6    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e1b04",
   "metadata": {},
   "source": [
    "# TestMultiply_NCHW_2_3_4_5_different_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0030f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TestMultiply_NCHW_2_3_4_5_different_dimension(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=(1,1), stride=1, padding='same', groups=1, bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d.bias)\n",
    "        self.conv2d_1 = torch.nn.Conv2d(in_channels=2, out_channels=2, kernel_size=(1,1), stride=1, padding='same', groups=1, bias=True)\n",
    "        torch.nn.init.zeros_(self.conv2d_1.bias)\n",
    "        self.max_pooling2d_ = torch.nn.MaxPool2d(kernel_size=(4,5) )\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=40, out_features=2, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv2d = self.conv2d(x)\n",
    "        y_conv2d_1 = self.conv2d_1(y_conv2d)\n",
    "        y_max_pooling2d_ = self.max_pooling2d_(y_conv2d_1)\n",
    "        y_multiply = y_conv2d * y_max_pooling2d_\n",
    "        y_flatten = self.flatten(y_multiply)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = TestMultiply_NCHW_2_3_4_5_different_dimension().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9, weight_decay=0.07)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([2, 3, 4, 5]),\n",
    "    y_numpy_array_for_tests(2, 2),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "assert abs(loss_before-0.6887038946151733) < 1e-6\n",
    "assert abs(loss_after-0.6871695518493652) < 1e-6    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b34fd8",
   "metadata": {},
   "source": [
    "# Test_SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Test_SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.bidirectional = torch.nn.RNN(input_size=1, hidden_size=2, num_layers=2, nonlinearity='tanh', bias=True, batch_first=True, dropout=0, bidirectional=True)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l0)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l0)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l1)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l1)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l1_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l1_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l0_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l0_reverse)\n",
    "        self.simple_rnn_1 = torch.nn.RNN(input_size=4, hidden_size=2, num_layers=1, nonlinearity='tanh', bias=True, batch_first=True, dropout=0, bidirectional=False)\n",
    "        torch.nn.init.zeros_(self.simple_rnn_1.bias_ih_l0)\n",
    "        torch.nn.init.zeros_(self.simple_rnn_1.bias_hh_l0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_bidirectional, y_bidirectional_hidden = self.bidirectional(x)\n",
    "        y_simple_rnn_1_return_sequences, y_simple_rnn_1_hidden_return_sequences = self.simple_rnn_1(y_bidirectional)\n",
    "        y_simple_rnn_1 = y_simple_rnn_1_return_sequences[:,-1,:]\n",
    "        return y_simple_rnn_1\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_SimpleRNN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([4, 2, 1]),\n",
    "    y_numpy_array_for_tests(4, 2),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.MSELoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "\n",
    "assert abs(loss_before-0.48370790481567383) < 1e-6\n",
    "assert abs(loss_after-0.2722862660884857) < 1e-6    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3166277b",
   "metadata": {},
   "source": [
    "# Test_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Test_LSTM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.bidirectional = torch.nn.LSTM(input_size=1, hidden_size=1, num_layers=2, bias=True, batch_first=True, dropout=0.25, bidirectional=True)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l0)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l0)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l1)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l1)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l1_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l1_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_hh_l0_reverse)\n",
    "        torch.nn.init.zeros_(self.bidirectional.bias_ih_l0_reverse)\n",
    "        self.lstm_1 = torch.nn.LSTM(input_size=2, hidden_size=1, num_layers=1, bias=True, batch_first=True, dropout=0, bidirectional=False)\n",
    "        torch.nn.init.zeros_(self.lstm_1.bias_ih_l0)\n",
    "        torch.nn.init.zeros_(self.lstm_1.bias_hh_l0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_bidirectional, y_bidirectional_hidden = self.bidirectional(x)\n",
    "        y_lstm_1_return_sequences, y_lstm_1_hidden_return_sequences = self.lstm_1(y_bidirectional)\n",
    "        y_lstm_1 = y_lstm_1_return_sequences[:,-1,:]\n",
    "        return y_lstm_1\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_LSTM().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([4, 2, 1]),\n",
    "    y_numpy_array_for_tests(4, 1),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.MSELoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 1,\n",
    "    batch_size = 2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b287723",
   "metadata": {},
   "source": [
    "# Test_DotProductAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e124a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Test_DotProductAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv1D_Q = torch.nn.Conv1d(in_channels=4, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_Q.bias)\n",
    "        self.conv1D_K = torch.nn.Conv1d(in_channels=4, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_K.bias)\n",
    "        self.conv1D_V = torch.nn.Conv1d(in_channels=4, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_V.bias)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=10, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv1D_Q = self.conv1D_Q(x)\n",
    "        y_conv1D_K = self.conv1D_K(x)\n",
    "        y_conv1D_V = self.conv1D_V(x)\n",
    "        y_scaleddotproductattention = F.scaled_dot_product_attention(y_conv1D_Q, y_conv1D_K, y_conv1D_V, attn_mask=None, dropout_p=0.0, is_causal=False, scale=1)\n",
    "        y_flatten = self.flatten(y_scaleddotproductattention)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_DotProductAttention().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9, weight_decay=0.07)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([6, 4, 5]),\n",
    "    y_numpy_array_for_tests(6, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-1.0957681735356648) < 1e-6\n",
    "assert abs(loss_after-1.0050253868103027) < 1e-6    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973468b",
   "metadata": {},
   "source": [
    "# Test_MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d827d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Test_MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv1D_Q = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_Q.bias)\n",
    "        self.conv1D_K = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_K.bias)\n",
    "        self.conv1D_V = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_V.bias)\n",
    "        self.multi_head_attention = torch.nn.MultiheadAttention(embed_dim=6, num_heads=3, dropout=0, bias=False, add_bias_kv=False, add_zero_attn=False, batch_first=True)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=12, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv1D_Q = self.conv1D_Q(x)\n",
    "        y_conv1D_K = self.conv1D_K(x)\n",
    "        y_conv1D_V = self.conv1D_V(x)\n",
    "        \n",
    "        # We build the attention mask 'attn_mask'\n",
    "        if not hasattr(self, 'attn_mask_dict'): self.attn_mask_dict = dict()\n",
    "        sz = y_conv1D_Q.size(1)  # L: Target sequence Length\n",
    "        \n",
    "        if sz not in self.attn_mask_dict: self.attn_mask_dict[sz] = torch.nn.Transformer.generate_square_subsequent_mask(sz, device = x.device, dtype = x.dtype)\n",
    "        y_multi_head_attention, _ = self.multi_head_attention(y_conv1D_Q, y_conv1D_K, y_conv1D_V, key_padding_mask=None, need_weights=False, attn_mask=self.attn_mask_dict[sz], average_attn_weights=False, is_causal=True)\n",
    "        y_flatten = self.flatten(y_multi_head_attention)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_MultiHeadAttention().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([10, 2, 6]),\n",
    "    y_numpy_array_for_tests(10, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "assert abs(loss_before-1.103563904762268) < 1e-6\n",
    "assert abs(loss_after-1.0815070867538452) < 1e-6    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b26f481",
   "metadata": {},
   "source": [
    "# Test_MultiHeadAttention_with_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01926011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Test_MultiHeadAttention_with_bias(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        self.conv1D_Q = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_Q.bias)\n",
    "        self.conv1D_K = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_K.bias)\n",
    "        self.conv1D_V = torch.nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1, stride=1, padding='same', bias=True)\n",
    "        torch.nn.init.zeros_(self.conv1D_V.bias)\n",
    "        self.attn_mask_dict = dict()\n",
    "        self.multi_head_attention = torch.nn.MultiheadAttention(embed_dim=6, num_heads=3, dropout=0, bias=True, add_bias_kv=False, add_zero_attn=False, batch_first=True)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.dense = torch.nn.Linear(in_features=12, out_features=3, bias=True)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_conv1D_Q = self.conv1D_Q(x)\n",
    "        y_conv1D_K = self.conv1D_K(x)\n",
    "        y_conv1D_V = self.conv1D_V(x)\n",
    "        \n",
    "        # We build the attention mask 'attn_mask'\n",
    "        sz = y_conv1D_Q.size(1)  # L: Target sequence Length\n",
    "        if sz not in self.attn_mask_dict: self.attn_mask_dict[sz] = torch.nn.Transformer.generate_square_subsequent_mask(sz, device = x.device, dtype = x.dtype)\n",
    "        \n",
    "        y_multi_head_attention, _ = self.multi_head_attention(y_conv1D_Q, y_conv1D_K, y_conv1D_V, key_padding_mask=None, need_weights=False, attn_mask=self.attn_mask_dict[sz], average_attn_weights=False, is_causal=True)\n",
    "        y_flatten = self.flatten(y_multi_head_attention)\n",
    "        y_dense = self.dense(y_flatten)\n",
    "        return y_dense\n",
    "\n",
    "device = 'cuda'\n",
    "model = Test_MultiHeadAttention_with_bias().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n",
    "loss_before, loss_after = Train(model,\n",
    "    numpy_array_for_tests([10, 2, 6]),\n",
    "    y_numpy_array_for_tests(10, 3),\n",
    "    device = device,\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss(),\n",
    "    optimizer = optimizer,\n",
    "    num_epochs = 10,\n",
    "    batch_size = 2\n",
    "    )\n",
    "\n",
    "\n",
    "assert abs(loss_before-1.1090373992919922) < 1e-6\n",
    "assert abs(loss_after-1.0816659927368164) < 1e-6    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
